{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-15T08:07:45.039435Z","iopub.status.busy":"2024-05-15T08:07:45.039046Z","iopub.status.idle":"2024-05-15T08:08:08.442916Z","shell.execute_reply":"2024-05-15T08:08:08.441413Z","shell.execute_reply.started":"2024-05-15T08:07:45.039403Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/dell/miniconda3/envs/torch/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n","  warnings.warn(_BETA_TRANSFORMS_WARNING)\n","/home/dell/miniconda3/envs/torch/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n","  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import torch \n","import cv2\n","import os\n","import torchvision\n","import random as rand\n","import numpy as np\n","from PIL import Image\n","from torch import nn, optim, utils\n","from torch.utils import data, tensorboard\n","from matplotlib import pyplot as plt\n","from torchvision import models\n","from torchvision.transforms import v2\n","from glob import glob\n","from sklearn.metrics import accuracy_score\n","torch.cuda.is_available()"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T08:08:08.445556Z","iopub.status.busy":"2024-05-15T08:08:08.444891Z","iopub.status.idle":"2024-05-15T08:08:14.511062Z","shell.execute_reply":"2024-05-15T08:08:14.509536Z","shell.execute_reply.started":"2024-05-15T08:08:08.445520Z"},"trusted":true},"outputs":[],"source":["imgs_dir = glob('oneshot/Face Dataset/**/*.jpg')\n","val_imgs_dir = imgs_dir[0:int(0.3*len(imgs_dir))]\n","train_imgs_dir = imgs_dir[int(0.3*len(imgs_dir)):len(imgs_dir)]"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T08:10:14.181503Z","iopub.status.busy":"2024-05-15T08:10:14.181050Z","iopub.status.idle":"2024-05-15T08:10:14.196156Z","shell.execute_reply":"2024-05-15T08:10:14.194885Z","shell.execute_reply.started":"2024-05-15T08:10:14.181473Z"},"trusted":true},"outputs":[],"source":["class FaceDataset(data.Dataset):\n","    def __init__(self, imgs_dir):\n","        self.img_dict = {}\n","        self.T = v2.Compose([\n","            v2.ToTensor(),\n","            v2.Resize(256),\n","            v2.CenterCrop(224),\n","            v2.Normalize(mean = [0.48235, 0.45882, 0.40784], std=[0.00392156862745098, 0.00392156862745098, 0.00392156862745098]),\n","            v2.RandomHorizontalFlip(),\n","            v2.RandomInvert(),\n","            v2.RandomRotation(degrees=127),\n","            v2.RandomVerticalFlip()\n","        ])\n","        for img_path in imgs_dir:\n","            img_data = img_path.split('/')[-2::]\n","            img = cv2.imread(img_path)\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","#             img = self.T(img)\n","            if img_data[0] not in self.img_dict.keys():\n","                self.img_dict[img_data[0]] = [img]\n","            else:\n","                self.img_dict[img_data[0]].append(img)\n","                \n","    def __len__(self):\n","        return len(self.img_dict)\n","    \n","    def __getitem__(self, index):\n","        face_anchor = rand.choice(list(self.img_dict.keys()))\n","        while True:\n","            face_imgs = rand.choices(self.img_dict[face_anchor], k=2)\n","            if ~np.array_equal(face_imgs[0],face_imgs[1]):\n","                anc = self.T(face_imgs[0])\n","                pos = self.T(face_imgs[1])\n","                break\n","        while True:\n","            face_neg = rand.choice(list(self.img_dict.keys()))\n","            if face_neg!=face_anchor:\n","                neg = self.T(rand.choice(self.img_dict[face_neg]))\n","                break\n","        del face_imgs\n","        return anc, pos, neg"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T08:12:37.480745Z","iopub.status.busy":"2024-05-15T08:12:37.480332Z","iopub.status.idle":"2024-05-15T08:12:37.490879Z","shell.execute_reply":"2024-05-15T08:12:37.489091Z","shell.execute_reply.started":"2024-05-15T08:12:37.480712Z"},"trusted":true},"outputs":[],"source":["# class EncoderCNN(nn.Module):\n","#     def __init__(self, ):\n","#         super().__init__()\n","#         self.vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n","#         self.vgg = nn.Sequential(*list(self.vgg.children())[0:-1])\n","#         self.drop = nn.Dropout(p=0.4)\n","#         self.linear = nn.Linear(in_features=25088, out_features=4096)\n","#         self.relu = nn.LeakyReLU()\n","        \n","#     def forward(self, img):\n","#         features = self.vgg(img)\n","#         bs, ch, hi, wd = features.shape\n","#         features = torch.reshape(features, [bs,-1])\n","#         features = self.drop(features)\n","#         out_lin = self.linear(features)\n","#         return self.relu(out_lin)\n","class ResLink(nn.Module):\n","    def __init__(self, in_ch) -> None:\n","        super(ResLink, self).__init__()\n","        self.con1 = nn.Conv2d(in_ch, in_ch*2, kernel_size=(3,3), stride=(1,1), padding=(1,1), bias=True)\n","        self.btn = nn.BatchNorm2d(2*in_ch)\n","    \n","    def forward(self, x):\n","        x = self.con1(x)\n","        x = self.btn(x)\n","        return x\n","        \n","class CNNBlock(nn.Module):\n","    def __init__(self, in_ch) -> None:\n","        super(CNNBlock, self).__init__()\n","        self.con1_1 = nn.Conv2d(in_ch, in_ch*2, kernel_size=(3,3), stride=(1,1), padding=(1,1), bias=False)\n","        self.btn1_1 = nn.BatchNorm2d(in_ch*2)\n","        self.rel1_1 = nn.ReLU()\n","        self.con2_1 = nn.Conv2d(in_ch*2, in_ch*2, kernel_size=(3,3), stride=(1,1), padding=(1,1), bias=False)\n","        self.btn2_1 = nn.BatchNorm2d(in_ch*2)\n","        self.rel2_1 = nn.ReLU()\n","        self.res_link = ResLink(in_ch)\n","        self.downsample = nn.Sequential(\n","            nn.Conv2d(in_ch*4, out_channels=in_ch*4, kernel_size=(3,3), stride=(2,2)),\n","            nn.BatchNorm2d(in_ch*4),\n","        )\n","\n","    def forward(self, x):\n","        x1 = self.con1_1(x)\n","        x1 = self.btn1_1(x1)\n","        x1 = self.rel1_1(x1)\n","        x1 = self.con2_1(x1)\n","        x1 = self.btn2_1(x1)\n","        x1 = self.rel2_1(x1)\n","        x2 = self.res_link(x)\n","        x = torch.cat([x1, x2], dim=1)\n","        x = self.downsample(x)\n","        return x\n","\n","class EncoderCNN(nn.Module):\n","    def __init__(self, in_ch, out_in_ch) -> None:\n","        super(EncoderCNN, self).__init__()\n","        self.con_in = nn.Conv2d(in_ch, out_channels=out_in_ch, kernel_size=(3,3), stride=(1,1))\n","        self.btn_in = nn.BatchNorm2d(32)\n","        self.rel_in = nn.ReLU()\n","        self.cnn1 = CNNBlock(out_in_ch)\n","        self.cnn2 = CNNBlock(out_in_ch*4)\n","        # self.cnn3 = CNNBlock(out_in_ch*16)\n","        self.avg = nn.AdaptiveAvgPool2d((1,1))\n","        self.out = nn.Sequential(\n","            nn.Dropout(),\n","            nn.Linear(in_features=out_in_ch*64, out_features=out_in_ch*16),\n","        )\n","    \n","    def forward(self, img):\n","        x = self.con_in(img)\n","        x = self.btn_in(x)\n","        x = self.rel_in(x)\n","        x = self.cnn1(x)\n","        x = self.cnn2(x)\n","        # x = self.cnn3(x)\n","        x = self.avg(x)\n","        x = x.squeeze()\n","        print(x.shape)\n","        x = self.out(x)\n","        return x"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self, ):\n","        super().__init__()\n","        self.pos = EncoderCNN(3, 32)\n","        self.neg = EncoderCNN(3,32)\n","    def forward(self, anc, pos, neg):\n","        out_pos = self.pos(pos)\n","        out_anc = self.pos(anc)\n","        out_neg = self.pos(neg)\n","        return out_pos, out_anc, out_neg"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/dell/miniconda3/envs/torch/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n","  warnings.warn(\n"]}],"source":["train_face_data = FaceDataset(train_imgs_dir)\n","train_face_data = data.DataLoader(train_face_data, batch_size = 4)\n","val_face_data = FaceDataset(val_imgs_dir)\n","val_face_data = data.DataLoader(val_face_data, batch_size = 4)\n","writer = tensorboard.SummaryWriter(log_dir='log')\n","n_epochs = 50\n","thresh = 0.9\n","step=0\n","criterion = nn.TripletMarginWithDistanceLoss()\n","model = Model().to('cuda')\n","adam = optim.Adam(model.parameters(), lr=3e-5)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T04:36:15.270805Z","iopub.status.busy":"2024-05-15T04:36:15.270519Z","iopub.status.idle":"2024-05-15T04:36:17.355625Z","shell.execute_reply":"2024-05-15T04:36:17.354252Z","shell.execute_reply.started":"2024-05-15T04:36:15.270779Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0 || "]},{"name":"stderr","output_type":"stream","text":["/home/dell/miniconda3/envs/torch/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 512])\n","torch.Size([4, 512])\n"]},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 3.81 GiB total capacity; 2.22 GiB already allocated; 42.94 MiB free; 2.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m anc, pos, neg \u001b[38;5;129;01min\u001b[39;00m train_face_data:\n\u001b[1;32m      8\u001b[0m     adam\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m     out_pos, out_anc, out_neg \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43manc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m     11\u001b[0m         out_anc, out_pos, out_neg\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m     tot_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, anc, pos, neg)\u001b[0m\n\u001b[1;32m      7\u001b[0m out_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos(pos)\n\u001b[1;32m      8\u001b[0m out_anc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos(anc)\n\u001b[0;32m----> 9\u001b[0m out_neg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_pos, out_anc, out_neg\n","File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[4], line 75\u001b[0m, in \u001b[0;36mEncoderCNN.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     73\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_in(x)\n\u001b[1;32m     74\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn1(x)\n\u001b[0;32m---> 75\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# x = self.cnn3(x)\u001b[39;00m\n\u001b[1;32m     77\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg(x)\n","File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[4], line 49\u001b[0m, in \u001b[0;36mCNNBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcon2_1(x1)\n\u001b[1;32m     48\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbtn2_1(x1)\n\u001b[0;32m---> 49\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel2_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_link(x)\n\u001b[1;32m     51\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x1, x2], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 3.81 GiB total capacity; 2.22 GiB already allocated; 42.94 MiB free; 2.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["for epoch in range(n_epochs):\n","    print(f\"Epoch: {epoch} ||\", end=' ')\n","    model.train()\n","    tot_loss = 0\n","    n=0\n","    j=0\n","    for anc, pos, neg in train_face_data:\n","        adam.zero_grad()\n","        out_pos, out_anc, out_neg = model(anc.to('cuda'), pos.to('cuda'), neg.to('cuda'))\n","        loss = criterion(\n","            out_anc, out_pos, out_neg\n","        )\n","        tot_loss+=loss.item()\n","        writer.add_scalar('Training Loss', loss.item(), global_step = step)\n","        step+=1\n","        loss.backward()\n","        adam.step()\n","        n+=1\n","    model.eval()\n","    same_dists = []\n","    not_same_dists = []\n","    print(f\"Loss: {tot_loss/n}\", end = \" \")\n","    with torch.no_grad():\n","        for anc, pos, neg in val_face_data:\n","            out_pos, out_anc, out_neg = model(anc.to('cuda'), pos.to('cuda'), neg.to('cuda'))\n","            same_dist = nn.CosineSimilarity()(out_anc, out_pos)\n","            not_same_dist = nn.CosineSimilarity()(out_pos, out_neg)\n","            same_dists.extend(same_dist.cpu().numpy())\n","            not_same_dists.extend(not_same_dist.cpu().numpy())\n","    avg_same_dist = sum(same_dists) / len(same_dists)\n","    avg_not_same_dist = sum(not_same_dists) / len(not_same_dists)\n","    print(f\"|| Validation Cosine Similarity - Same: {avg_same_dist} || Not Same: {avg_not_same_dist}\")"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"text/plain":["EncoderCNN(\n","  (con_in): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n","  (btn_in): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (rel_in): ReLU()\n","  (cnn1): CNNBlock(\n","    (con1_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (btn1_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (rel1_1): ReLU()\n","    (con2_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (btn2_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (rel2_1): ReLU()\n","    (res_link): ResLink(\n","      (con1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (btn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (downsample): Sequential(\n","      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n","      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (cnn2): CNNBlock(\n","    (con1_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (btn1_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (rel1_1): ReLU()\n","    (con2_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (btn2_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (rel2_1): ReLU()\n","    (res_link): ResLink(\n","      (con1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (btn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (downsample): Sequential(\n","      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n","      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avg): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (out): Sequential(\n","    (0): Dropout(p=0.5, inplace=False)\n","    (1): Linear(in_features=512, out_features=512, bias=True)\n","  )\n",")"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["model.pos"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["(2461, 5743, 8204)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["len(val_imgs_dir), len(train_imgs_dir), len(imgs_dir)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["img_dict = {}\n","for img_path in imgs_dir:\n","    img_data = img_path.split('/')[-2::]\n","    # img = cv2.imread(img_path)\n","    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    if img_data[0] not in img_dict.keys():\n","        img_dict[img_data[0]] = [img_path]\n","    else:\n","        img_dict[img_data[0]].append(img_path)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/dell/miniconda3/envs/torch/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n","  warnings.warn(\n"]}],"source":["T = v2.Compose([\n","            v2.ToTensor(),\n","            v2.Resize(256),\n","            v2.CenterCrop(224),\n","            v2.Normalize(mean = [0.48235, 0.45882, 0.40784], std=[0.00392156862745098, 0.00392156862745098, 0.00392156862745098])\n","        ])"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["similarity = nn.PairwiseDistance()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def validation(img_dict):\n","    while True:\n","        img_1 = rand.choice(list(img_dict.keys()))\n","        img_2 = rand.choice(list(img_dict.keys()))\n","        if img_1!=img_2:\n","            img_1_list = rand.choices(img_dict[img_1], k=2)\n","            img_2_list = rand.choices(img_dict[img_2], k=2)\n","            if img_1_list[0]!=img_1_list[1] or img_2_list[0]!=img_2_list[1]:\n","                break;\n","    img_1 = [model.pos(torch.unsqueeze(T(cv2.imread(img)).to('cuda'), dim=0)) for img in img_1_list]\n","    img_2 = [model.pos(torch.unsqueeze(T(cv2.imread(img)).to('cuda'), dim=0)) for img in img_2_list]\n","    return img_1, img_2"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([512])\n","torch.Size([512])\n","torch.Size([512])\n","torch.Size([512])\n"]}],"source":["img1, img2 = validation(img_dict)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["img1_1, img1_2 = img1\n","img2_1, img2_2 = img2"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(3.1944, device='cuda:0', grad_fn=<NormBackward1>)"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["similarity(img1_1, img1_2)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(5.8567, device='cuda:0', grad_fn=<NormBackward1>)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["similarity(img1_1, img2_1)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(4.6384, device='cuda:0', grad_fn=<NormBackward1>)"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["similarity(img2_1, img2_2)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(1.9392, device='cuda:0', grad_fn=<NormBackward1>)"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["similarity(img2_2, img1_2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":1365967,"sourceId":2415961,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
